[{"title":"Homepage of Fergus Fettes","date":"2022-10-18T20:17:56.700Z","url":"/2022/10/18/hello-world/","content":" “Salamanders have their appointed Dwelling in Fire; Sylphs in the Air; Nymphs in the flowing Waters; and Gnomes in Earthen-burrows, but the creature whose substance is Bliss is everywhere at home. All sounds, even to the roaring of Lions, the screeching of the nightly Owls, the laments and groans of those entrapped in Hell, are as sweet Musick to her. All odours, even to the foulest stench of Corruption, are to her as the delight of roses and Lilies. All savours, even to the banquet-table of the Harpys of heathen lore, are as Sweet loaves and spiced Ale. Wandering at noon through the Waste-Places of the world, it seems to her she is refreshed by Canopies of flocking Angels. The earnest seeker will look for her in All places, however dim and sordid, of this world or in the seven others. Thrust a keen Sword-blade through her and it will \\seem as a fountain of Divine and Pure pleasure. \\These eyes, by Translation, have been given to see her ways; and an equal gift as revealed by Wisdom is sometimes granted the Child.” ~ Jane Lead1 ReviewsI have started reviewing things, cities first. This is a largely biographical account of places I have been and what I thought about them. I will add more as time goes on (I hope!) 1.My subtitle is plagiarised from the Borges book where I was introduced to her. I have not been able to track down the original source for this, despite some attempts, so it could of course be a Borgesian invention. ↩"},{"title":"Paint in VR","date":"2021-02-06T22:10:29.000Z","url":"/2021/02/06/oculus-paint/","content":"BackgroundI bought myself a VR headset. My justification was some trailers for ‘Infinite Office’, and my love of portability– if it is true that you can have 5x screens from one laptop strapped to your head, then it’s really a gamechanger in terms of portability. As it happens, I enjoy the office space (though there are still many kinks to be worked out, naturally– for instance it doesn’t support multiple screens yet on Linux), but I am not going to talk about Oculus for work right now. The second thing that I bought it for, was to encourage myself to get back into JavaScript development. I consider JavasScript development to be quite unpleasant, I don’t like the tools or the workflows, but it is what so much of our life is built on, so I want to continue learning more until I am at least at a sort of journeyman-competency. So here is my journey to making a small app in JavaScript-VR. Getting StartedFirst I went over my old JS experinements&#x2F;work, refactoring a little and refamiliarising myself with the concepts. I ran some of my old 3D geometry monstrosities, and signed up as an oculus developer. The first hurdle I hit was serving my creations to the headset. WebXR only works over https. But there are built-in execptions for localhost, so I had to get the Oculus to think it was at my localhost– sounds like a job for port forwarding. I started with the oculus chrome debugging guide. I find myself using something called ‘adb’– I assume this is the android debugger. This is exciting! I’m now an adroid developer apparently. I found the IP of the machine and tried to ssh in: but to no avail. A moment or two googling later I found the command: and we’re away! I can now open locally hosted sites on my Oculus! And I imagine I have also learned some stuff that will be useful for using my phone as a webcam or other such tricks, very excellent. Copying CodeI first try running some of the ThreeJS examples locally, then I pick the one I want, refactor it (I hate HTTP and JS in one document, and I tend to backup all the ThreeJS files I am using in my own src file to avoid too much dependency hell when building), and commit it to my new javascript playground. Fortunately, their demo is very limited in scope. This is fortunate because I gives me something to do! It only uses two triggers for everything– you can paint, and resize your tool. I will add some buttons. To learn the correct names for the events, I’m copying this example from the lovely list of WebXR Samples. In fact I’ve cloned the whole repo so I can read through it. DebuggingReading through lots of WebXR and ThreeJS libraries, I’m still not sure how to mesh them together– specifically I can’t figure out what the button-press events are called. So I’m going back to the debug guide to actually catch the events as they are happening. So now I have chrome on my laptop debugging the session in the Oculus. Snazzy. I try the Event Listener Breakpoints but no dice. Maybe its too new of a feature? Anyway I stick a breakpoint in the render loop and try to trigger it– my headset only functions when you are wearing it, so I have to have one eye in the headset and one the debugger, but eventually I manager to capture an event. I’ll eventually try to find a way to leave the headset running when I’m not wearing it. From that captured event I get a nice log dump of all the features– So that is something to work with! ProgressBefore tryig to integrate any of the fancy stuff from the WebXR example page (fancy just meaning ‘basic graphics’, but they write all the vertices themselves so it looks a lot more than some simple ThreeJS stuff), I decided to make a quick sanity check so I can really set stuff up to work nicely. So here I am just dumping all the events from the gamepads into the console: I start it up and it works! My Oculus controller has buttons 0, 1, 4 and 5, and 3 is the joystick click. I assume 2 is reseved for the oculus button on the right controller, though maybe I can use it on the left one. It also has sensitive axes on buttons 0 and 1, and the joystick. Plus position information! A veritable wealth of possibilities. I’ll continue with this tomorrow. ArchaeologyI spent an age trying to add colors to the TubePainter module. I got very confused by it because I couldn’t believe that they hadn’t added the functionality to change the color, but I guess the project is really a stub. In fact the more I look into VR development the puzzles I am spotting. I looks like there was a major boom in VR about five years ago, most of the libraries I’m working with were started back then. But then they seem to have somewhat petered out. Maybe we are currently in a little VR-winter, as the initial hype overshot all reality. Maybe we are heading into a thaw now. Lets see. Furthermore, notes after a day or two of regular Oculus useage– it definitely takes a toll on the eyes. Everything has a somewhat hallucinagenic quality when you come out of it, like an acid comedown. I’m not sure if this effect will diminish or grow with continued usage. I can see it being very problematic for people who suffer from HPPD. Or maybe all the Oculus Developers are elsewhere, working with the Unreal Engine or Unity or someeuch. Maybe ThreeJS isn’t cool. But I must say I am impressed by how much this thing can run, I spend 20 minutes filling my vision with layer upon layer of multicoloured swirls, without noticing any drop in quality or responsivness. The next part I have to integrate is the VR answer to the dat-gui library. Also made 4 years ago. FrustrationOnce again I am reminded of how much I hate the JS workflow. I need to improve this somehow. With the Oculus it is particularly bad– make a change, serve the change, pick up headset, wait for it to start up, then when it is runnign start up the debugger with a keyboard shortcut, then take it off and do some debugging. Repeat. I will eventually work on a better workflow by integrating RiftSketch. That will be a joy I think, if I can code and manipulate objects all in VR. That is the goal. FeaturesSo after an age of fiddling with GUIs I gave up for the moment and satisfied myself with two new features: random color generation (with or without changing the color of your little cloud of cubes) buffer deletion (specific to each hand, so you can delete work in one hand only) Obviously both of these things can be much upgraded, a C-z feature and better color choosing. But I think it is enough to have some fun with just now so I will push it to github so I can use it untethered. I’ll make it available at paint.fergusfettes.com. Further InvestigationsSo I played with RiftSketch. It is wonderful. It just needs a propery editor. I had a look around the internet (even asked on reddit) then found what I needed. In the process I stumled across some other stuff that might come in useful. At the end of the day what I need is this: Vim Javascript Implimentation for my editor RiftSketch for the framework ThreeJS Editor tools stolen from here. Actually maybe I can just skip RiftSketch and add Vim and a VRButton directly to that. But I would feel bad if I left RiftSketch behind. All of this mixed with my paint program of course! It’s going to be mental. In all seriousness, in the short term, if I can integrate that JS Vim implementation into RiftSketch I will be a happy man. This process is teaching me a great deal about JavaScript. That should go without saying. In the last hour I learned: how to use prettier and eslint to lint my stuff how to reverse engineer files from .maps more about building&#x2F;packaing etc (still trying to use parcel for everything since it is nice and minimal) "},{"title":"Streaming Oculus","date":"2021-02-06T22:10:29.000Z","url":"/2021/02/06/oculus-streaming/","content":"BackgroundOculus streaming is still a bit of a shitshow. Or, maybe thats unfair, but its very facebook. Streaming works where they want it to work (eg. your newsfeed). I want to stream to radio.schau-wien.at, where I am experimenting with livestreaming&#x2F;broadcasting. So lets get back into android development! I’m on a different laptop now so I have to relearn everything from last time. My devices are gone, my permsission are gone. Lets see. :sad: Okay, turns out to become an android developer you literally have to tap the build number of your phone seven times. It even gives you little hints like ‘you are now 4 steps from becoming a developer!’. Nice little easter-egg. Okay and heres I one-liner a ran directly from SO: Daamn that was easy."},{"title":"Mirroring in the Hyperspace and the IPFS","date":"2020-12-29T17:03:06.000Z","url":"/2020/12/29/distributed-mirrors/","content":"I’m going to mirror my blog in Hyperspace (aka the backend of the Beaker browser, if you don’t know about it I suggest you check that out first). To do this, I’m going to try setting up my free micro instance[^micro] in the gcloud as a hyperdrive host. Hyperdrive HostI’ve had trouble doing big installs on a micro instance before. For reference, here are the specs of a micro instance: RAM Cores Mem 0.6GB 0.2 (aka a shared core!) 32GB I run my ansible script against it, and it dies. I try to install and run a docker image, it dies. It is not made for grand works. Or even minor works. So in order to get everything nicely installed and prepared I decided to make a ‘standard simple computer‘. This is a decently-proportioned machine I can run in the cloud for generating snapshots that can be used as the base image for smaller, gentler machines. It does all the installations without crashing. I also used the opportunity to try out a preemptible instance. This is an instance that google can kill whenever, and is about 3x cheaper than a normal machine. Since I am just running this machine to do little tasks in the cloud, I don’t care if it gets killed sometimes. However, I didn’t realise that my free cloud credits can’t be used for preemptible instances, I only found this out after. This means– shock horror! I will actually get billed for this usage. Lets see how much. SKU Service Usage Cost Preemptible E2 Instance Core running in Americas Compute Engine 1.63 hour €0.01 Preemptible E2 Instance Ram running in Americas Compute Engine 6.5 gibibyte hour €0.00 Okay not too terrible. So now I have a snapshot with some basic things installed, I can try running a hyperdrive host. Obviously I don’t need all my command-line tools and everything to really run it, but its nice to have this snapshot available for future machines. And when I get it all running stably, I can create a minimal snapshot with only the server I want so it runs more smoothly. To make my hyperdrive, I’m just going to open up the Beaker browser and see how it is these days, it’s been almost six months since I used it and it’s developing rapidly. Oh! They have a ‘create hyperdrive from git repo’ button now! This gets better and better. So I’m now hosted on the Beaker network. URL. Okay so it seems the way Hugo generates sites, based on the CNAME that I supplied, means that all the links from the frontpage on beaker go straight to the clearnet version at www.fergusfettes.com. This is slightly annoying but I’ll not work on it now, since all the rest of the stuff is actually there, it just isn’t linked correctly. This post on the hyperspace, for example. Another thing to note, is I would need to host all the images in hyperspace too to be really complete. However, as it is now, it is a great demonstration of how nicely the hyper protocol interacts with http. So now I’m restarting my micro instance with the hyperdrive installed. Starting hyperspace (hyperspace start), and hyperdrive (hyperdrive start). Playing around with the hyperdrive cli, I can get info about my local hyperdrvie once I have cd’ed into it: cd Hyperdrive; hyperdrive info --root. Now I’ll try mounting the hyperdrive that was created on my laptop by the Beaker browser. I copy the URL ID (aka. without the ‘hyper:&#x2F;&#x2F;‘) and serve it with (after the mount there is handy output telling you what to do next :D). Having a look around the beaker user list, I found my first attempt at a beaker blog is still being hosted (I’m @frgnym)! I feel so touched! Someone out there is still on my website! I’ll seed it too: Okay awesome. I’m not only newly mirrored on the hyper network, my old website is still on there. So great. That was incredibly easy. Maybe I’ll start using the beaker browser as my primary browser. (I always say that.) So now my old website is hosted in three places, whoever was hosting it before, my micro instance, and the browser I am currently viewing it from. See the little three in the top right: And now this blog is hosted there too! Excellent. Now lets see how the IPFS fares! UPDATE: It was obviously Kicks Condor who was hosting it before, so I’m hosting their drive now too. UPDATE2: So, the micro host wasn’t actually working. I ran a couple tests, turns out I used the complete wrong command. I should have used the following: The random name doesn’t seen to affect the seeded drive. ‘hyperdrive mount X’ creates a new drive called x and creates a new key for it, so I had folders named after all my old drives with new keys and no content. Anyway I fixed all the mounts and now I see the IP of my micro host in the top right under the hosts. In other news, this means that there were three people hosting my old blog before, and now there are 4! IPFS HostOkay so I’ve started my small computer again and I’m going through the Command-line quickstart. Seems pretty straightforward. So I dumped the contents of my dotfiles into my local IPFS node (ipfs add -r ~/dotfiles), and now I’m going to serve it and mount it. I ran the daemon (ipfs daemon) and set up an ssh tunnel to my small computer ssh -fNL 5001:localhost:5001 small-computer so I can see the IPFS dashboard at localhost:5001. Very snazzy! I can see the files I added too. Now I’ll get the browser plugin that lets me view IPFS files in Firefox. Actually since the install was so easy, I’ll run an IPFS node on my local laptop too. When I add my dotfiles locally, I notice that they have the same hashes as when I added them on the server. This is key to the coolest feature of the IPFS– since these files had already been added by me elsewhere, they didn’t get added again, and when I tried to add them locally the network determined this and just ‘pinned’ them as well, aka my local node automatically became a seeder of these same files. Network deduplication. This is a game-changer. Particularly in a world where things like the pypi and npm exist, where vast numbers of developer-hours are spent redoing work, it is easy to see how the notion of deduplication can be expanded to do more and more abstract kinds of energy-saving work. (Imagine writing tests for kinds of work you want code to do, and the code itself being mutable. If multiple people had done work that passed those tests, you could choose between them based on other factors like speed, efficiency etc.) So anyway. Now I go to my blog, and do ipfs add -r public[^git], and hey presto, my blog is in the IPFS. Running on my local machine, that link redirects to the following: so it just gets the file from the local folder nice and easy. However, when I go to the same link in Chrome (where I havent installed the plugin) it loads as well without a redirect (after some time). So I think the ipfs.io acts as a temporary node, grabbing the hash you request from the network and serving it over http. Aka, it came to my local node, got the file, then gave it back to me. Now, the blog in the IPFS has the same issue as the one in hyperspace– all the links point to the clearnet. However this is still fine for now, and still shows nicely how the two protocols interact. So now I just need to set up the host in my micro instance, and I’m done for the day. I’m going to take a risk and try to do it directly in my micro host. It’ll probably crash and die but lets see. I shut down some unnecessary services on the host. I’ve inited the ipfs node. And added the blog again. And started the daemon. And the node is still accessible. I shut down my local node and tried the URL again through the ipfs gateway and.. its there! I’m a little worried it’s just being cached by the ipfs gateway but I’ll check back tomorrow and see. Nice! I’m now mirrored on two decentralized internet protocols! I’m safe in the case of nuclear apocalypse! I’m part of the future! ConculsionsVery cool to have finally actually used both of these protocols, I have been following and stanning them for ages now. They both exceed expectations in terms of user-friendliness and just-works-ness. Maybe thats just because I’m a programmer now so things are easier, but their install and setup instructions are really nice and clear and worked without a hitch in this case. Obviously there are still some things I need to sort out, and some things I want to try: Set up cronjobs to update all three of the hosts once per day with the latest content (I know I should use build pipelines but I have never used github pipelines and I think a little cronjob is fine for now). See if it is possible to have relative paths that work in all three hosts, so internal links work properly and always link within the same protocol. Have a look at hyper-ipfs inter-operability. Set up nodes on a raspberry pi and serve from home. In terms of updating, becaue I just dumped whatever blog I had at the time into the protocol, the version in hyper doesn’t have this post at all, and the version in IPFS has this post without the IPFS content. This needs to be fixed first. The other things will take more time and tweaking so I’ll put them on the back burner for now. So! Great success! I’m off to a New Years bonfire now. Lets burn the bad 2020-vibes away. [^micro]: With the Google Cloud Platform free developer package, you get a teeny little machine in the cloud for free forever.[^git]: I made a point of checking if hidden files are added, they aren’t by default so the .git folder should be skipped."},{"title":"Music-I-Like","date":"2020-12-26T15:01:39.000Z","url":"/2020/12/26/music-i-like/","content":"IntroI like listening to music. I try to listen widely, but also (somewhat) deeply, getting into niche genres and exploring the landscapes I find there. I had a quite tepid entry into music appreciation, growing up listening to Leonard Cohen, Bob Dylan, the Red Hot Chili Peppers, and whatever happened to be popular at the time among my peers (I’m of the Limp Bizkit generation so you can image what that means). While I was at university, in my early 20s, my musical horizons were broadened, and continued to broaden throughout my twenties. I started listening more and more widely not only to ‘popular’, contemporary music, but also taking a fairly slow, broad pass through the history of classical music. Below I have summarized some of my listening habits of the past ten years. SpotifyI have been using spotify for almost ten years now. Back in the day I used to like songs and make playlists, but for the past 7 years or so I have listened almost exclusively to albums. I curate my downloaded albums quite closely, deleting things that become too familiar (though there are many guilty pleasures that I download and delete again and again) and always making sure I have a good collection of unfamiliar music downloaded. To deal with the fact that I always deleted music I know, I realised I needed a hardcopy of my listening habits. So I got a little book in which I would periodically write down the best music I had been listening to recently. Sometimes I would forget to update it for quite a while, but generally every month or two I would write down some highlights. In the front of the book I wrote down ‘classical’ music, in the back ‘popular’ albums. I recently digitized these lists^classical list on my journey home (a disasterous train journey from London during a Tier 4 lockdown). Looking at them, I feel like they are strangely sparse and sad. I love listening to music and do it almost incessantly. Are these two lists really representative of what feels like such a significant part of my life? In an effort to delve further, I also exported my old Liked&#x2F;Starred Songs playlist^old likes. Cringeworthy as some of my old taste is, I feel like this works better as an active snapshot of listening, and I recently started starring songs again. My intention with my starred list is not to create a playlist that I would particularly want to listen to, but to keep a record of songs from albums I like. So every song on my new list should been seen as a pointer to an album (though they are of course normally my favourite or the most catchy song). I will export and update this list regularly^likes 2020&#x2F;2021, and probably rotate every year. Of course Spotify is only a small part of my listening. Here I will list some other sources. YoutubeYoutube is a fantastic source for curated piano music. This is my main reason to go there. Here are some of the best channels I’m aware of: Ashish Xiangyi Kumar The classic ‘wonderfully curated piano music with blurbs and sheets’. Long may Xiangyi rule the channels. Medtnaculus Another good one. Flavi Chan Samurai Music Investigator Freaky music from all over the world with a strong oriental inclination. Some of my favourite discoveries are either from here or present here (Miharu Koshi, Haruomi Hosono, this track which has to be one of my favourite grooves of all time). Youtube is also a great for really weird stuff that wouldn’t show up in most musical catalogues, like a 20-minute long piano interpretation of Kavinskys Outrun which I once saw and (to my enduring regret) didn’t immediately download, or fantastic pieces like everywhere at the end of time or this 5-and-a-half hour long piece trying to get you used to a different tuning method. Or shaky-cam home footage of unusual choirs from around the world. Also, I have a weakness for Anthony ‘Best Teeth in the Game’ Fantano and have gotten several recommendations from him. I can’t claim that this is the most profound source in the world but he does fine work nevertheless. I also watch Adam Neely sometimes, but not for his reccis. Minor shoutout. VK (Russian Facebook)Thanks to their very relaxed copyright policies, VK is a great place to find wonderfully well curated collections on particular niche topics, particularly stuff that you might struggle to find elsewhere. Personally, I was using it to explore Japanese pop music from the 70s and 80s (City Pop, see here for an example). Yamashitter, properly tagged jpop and vintage jpop are a good place to start. However their ‘Awesome Tapes From Africa’ section is also great. (I mean you could also just follow that record label of course.) OtherSoulseekI haven’t gotten much into it yet but I hear it is the dream for things that can’t be found elsewhere. SoundcloudI only use Soundcloud to listen to this one song that was produced by a friend of a friend, and my old partners solo stuff. I should really dive in there for the really cutting edge stuff, I know. But I tend to listen to much more older and more established music than the state-of-the-art. That is neither to my credit nor discredit in my opinion. LiteratureOccasionally I want to here some opinions about the music I like. I generally look up whatever album I’m considering on Pitchfork on these occasions. Other than that, there are two sources I should mention: Piero Scaruffi, a physicist I think? I have no idea how I discovered that list, but I have gotten some good stuff from there. And finally, Ted Gioia. I am working through that list just now. Having gotten halfway through 2019, I’m realising that I need to start skipping the jazz and the folk singers that he recommends, but there is so much weird and fascinating stuff from all over on those lists that I think he will continue to be a great source for many years to come. Conculsions and AcknowledgementsI would like to thank the youtube algorithm and the many many youtube channels that I cannot credit because I have never known them, for their tireless work. I think it is very likely that the sources credited above (which the exception of ‘spotify, generally’) actually make up a small proportion of what I listen to, with much more coming from diverse sources in the youtube or spotify algorithmic radio. I would like to thank ublock origin for making youtube bearable. (I pay for Spotify, but do not do so with the mistaken believe that that constitutes supporting artists.) Finally, I would like to acknowledge that in fact, most of my musical journey (at least in contemporary music) has been driven largely by the fact that I have had the very good fortune to live with people who have been various degrees of music journalist&#x2F;musician&#x2F;music obsessive, and it was these people who set me on my way and kept me on the straight and narrow; in approximate order of when they started supplying me regularly with reccis: Kevin Mulligan Hanna Lemmik Diarmuid Morgan Emma Ludwig Felix Opper Sophie Nam Koong And special mentions to Felix Mulligan, my brother Max, and Gesa Maass for contributions."},{"title":"Alacritty Ascii","date":"2020-12-10T20:56:02.000Z","url":"/2020/12/10/alacritty-ascii/","content":"Alacritty is one of the many lovely developer tools that are springing up as the first flush in the Rust revolution. Its a fantastic terminal that I’m sure I will have more to say about in due time. As noted, I discovered something lovely while transferring some files the other day. I was moving my small film collection (currently stretching to 7 titles: &lt;INSERT FILMS&gt;) from one computer to another, and I just wanted to sanity check that the transfer had succeeded. So I sshed into the machine ssh work and ran vlc -I dummy ~/films/Mullholland\\ Dr.mkv. Imagine my surprise to see the criterion logo appear, animated before me. Here is a nice still from another film to illustrate: You can also look at a copy-paste of the terminal here to prove that it really is ASCII output. Zooming out, the resolution becomes better and better (text), but it starts to jitter quite badly at a certain size. I thought the ssh connection might be a bottleneck, so I went to the computer to try it on one machine. I had that slightly haunted feeling that the magic might disappear, and when I entered the terminal I suddenly wondered How will I get it to work here? If I run vlc -I dummy ~/filmes/Crash.mkv I’ll just get a window. but as soon as the thought was there I realised I could just start with ssh localhost. And it worked. I was watching the ‘Universal’ logo barge into my terminal. But things got even better. I noticed that my keystrokes were even getting sent back to vlc, so I could skip forward, pause and even quit the film (sometimes it gets stuck and I have to killall vlc but so it goes). After a little sleuthing, I notice that is interactive, but is not. What is this dummy terminal doing after all? Well, the vlc wiki has a little info^vlc dummy, and they nicely point out that keystrokes are sent back. I find the terminology a little odd, since ‘dummy video output’ is just normal video output, but I think they are looking at it from a gui-makers perspective. And this seems to be the key– with the dummy interface, vlc streams the movie to a window. Alacritty is presumably pretending to be a screen or a window of some sort, in order to pass its work to the GPU, so VLC merrily dumps the video stream in there. What I would love to know, and I might take some time to investigate, is how is Alacritty mapping the video stream to ASCII? And why? I will leave this story here for now. I have two things in mind: one, to investigate further over at the Alacritty github and two, to have some people round for a film, run it through Alacritty-ASCII and run the audio through a digital soundcard or some synths, and make a little performance. I’ll update if either of these things transpire. In the meantime, if you want to try it yourself, I have uploaded a slightly rushed video of the effect in action (the size limit on asciinema was not made with 4k ASCII movies in mind, so no hard feelings). Asciinema cast attached^cast. And here is a final zoomed in still to admire. In the attached text you can see how much of a difference the color makes, presumably a lot of the 8s have the same foreground and background color. "}]